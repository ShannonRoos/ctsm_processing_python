{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85aba354-549e-409d-9d65-c89581473a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import HS_functions\n",
    "import LAI_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225b483-26da-42c5-9937-3bc8757de276",
   "metadata": {},
   "source": [
    "#### 1. load in LAI (GLASS, CGLS, CLM5_exp) + temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac2ede4-2220-4b0f-b9fb-f48239a6f644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/sroos/tmp/ipykernel_15322/268707620.py:5: UserWarning: Converting non-nanosecond precision datetime values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time.\n",
      "  ds_TSA      = ds_TSA.assign_coords(time=(\"time\", np_time))\n"
     ]
    }
   ],
   "source": [
    "# temperature data from air max tenoerature \n",
    "ds_TSA      = xr.open_dataset(f'ihist.e52.IHistClm50BgcCrop.f19_g17.2.HSF_apsim/Temperature/T_1990_2010.nc') \n",
    "cftime_time = ds_TSA.time.values\n",
    "np_time     = np.array([np.datetime64(str(t)) for t in cftime_time])\n",
    "ds_TSA      = ds_TSA.assign_coords(time=(\"time\", np_time))\n",
    "\n",
    "# Subtract one day from each timestamp\n",
    "corrected_time = np_time - pd.Timedelta(days=1)\n",
    "\n",
    "# Assign corrected time back to dataset\n",
    "ds_TSA      = ds_TSA.assign_coords(time=(\"time\", corrected_time))\n",
    "TSA         = ds_TSA['TVDAY'] - 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b5ef6c-2b51-43bf-b2a4-33ad700befaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAI Reference\n",
    "ds_ref  = xr.open_dataset('GLASS_LAI_netcdf/LAI_masked_2deg/GLASSLAI_CCImasked_10pct_2deg_1981_2014.nc')\n",
    "ref_lai = ds_ref['LAI']\n",
    "ref_lai = ref_lai.assign_coords(lat=TSA['lat'])\n",
    "years   = sorted(set(pd.to_datetime(ref_lai['time'].values).year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf23731-dee4-4eba-8473-ef80a287fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAI CLM\n",
    "f_clm       = f'ihist.e52.IHistClm50BgcCrop.f19_g17.2.HSF_exp0/ELAI_90_14.nc'\n",
    "ds_clm      = xr.open_dataset(f_clm)\n",
    "cftime_time     = ds_clm.time.values\n",
    "np_time         = np.array([np.datetime64(str(t)) for t in cftime_time])\n",
    "clm_lai         = ds_clm['ELAI']\n",
    "clm_lai         = clm_lai.assign_coords(time=(\"time\", np_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3576a043-1f0c-41e7-b5e0-7688b5b4bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align years\n",
    "ref_lai   = ref_lai.sel(time=slice(ds_TSA.time.min(), ds_TSA.time.max()))\n",
    "clm_lai   = clm_lai.sel(time=slice(ref_lai.time.min(), ref_lai.time.max()))\n",
    "TSA       = TSA.sel(time=slice(ref_lai.time.min(), ref_lai.time.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf3149-b760-4827-9fd2-9dbdd9e3bcb0",
   "metadata": {},
   "source": [
    "#### 2. define conditions for AUC and HS period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd17a5-b0d3-4caf-bf81-59b0fb53f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate heatwave periods:\n",
    "HW = HS_functions.HW_mask(TSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763ea76-31c6-4ef8-b813-853e576533bd",
   "metadata": {},
   "source": [
    "##### calculate mean reference LAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e808aaab-df13-425d-8681-ba3db1a74e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calc_cgls_slopes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#go from dekads to daily data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m LAI_ref_daily        \u001b[38;5;241m=\u001b[39m ref_lai\u001b[38;5;241m.\u001b[39mresample(time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minterpolate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m#LAI_CGLS.resample(time=\"1D\").bfill()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m LAI_ref_slopes_daily \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_cgls_slopes\u001b[49m(LAI_ref_daily)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# # Apply a 15-day rolling mean centered on each day\u001b[39;00m\n\u001b[1;32m      6\u001b[0m LAI_ref_mean     \u001b[38;5;241m=\u001b[39m LAI_ref_daily\u001b[38;5;241m.\u001b[39mrolling(time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, center\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mconstruct(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calc_cgls_slopes' is not defined"
     ]
    }
   ],
   "source": [
    "#go from dekads to daily data\n",
    "LAI_ref_daily        = ref_lai.resample(time=\"1D\").interpolate(\"linear\")   #LAI_CGLS.resample(time=\"1D\").bfill()\n",
    "LAI_ref_slopes_daily = calc_cgls_slopes(LAI_ref_daily)\n",
    "\n",
    "# # Apply a 15-day rolling mean centered on each day\n",
    "LAI_ref_mean     = LAI_ref_daily.rolling(time=15, center=True, min_periods=1).construct(\"window\")\n",
    "\n",
    "# # Convert time to day-of-year (DOY) of a no-leap year\n",
    "doy_ref              = LAI_ref_mean.time.dt.dayofyear\n",
    "year_ref             = LAI_ref_mean.time.dt.year\n",
    "LAI_ref_mean         = LAI_ref_mean.sel(dayofyear=LAI_mean_HS.dayofyear != 60) ! check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7eaf98-958a-4c84-8d14-1df8d33cb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean for each day of the year across all reference years\n",
    "mean_LAI         = LAI_ref_mean.groupby(adjusted_doy).mean(dim=(\"window\", \"time\")).rename({'group': 'dayofyear'})\n",
    "mean_LAI_diff    = mean_LAI.diff('dayofyear')\n",
    "\n",
    "#get slope mean LAI\n",
    "mLAI_mask             = np.where(((mean_LAI_diff < 0 ) & (mean_LAI> 0.1 * mean_LAI.max()) ), 1,0)\n",
    "mean_lai_filtered     = mean_LAI[:-1,:,:] * mLAI_mask\n",
    "mean_LAI_slope        = mean_lai_filtered.where(mean_lai_filtered  > 0)\n",
    "mean_LAI_slope_masked = slope_filter(mean_LAI_slope,8)\n",
    "\n",
    "\n",
    "# get doy window for mean_LAI\n",
    "#date of max slope\n",
    "mean_LAI_max_date = mean_LAI.idxmax(dim=\"dayofyear\", skipna=True)\n",
    "mean_LAI_max_date_plus35 = ((mean_LAI_max_date + 35 - 1) % 365) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7c5b4-3119-4989-8bc3-c188799ebc56",
   "metadata": {},
   "source": [
    "#### count valid sequences reference LAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512b8ab-a8f4-4dde-85b5-04bcb59839ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count valid sequences:\n",
    "# Apply function over lat/lon grid\n",
    "seq_count = xr.apply_ufunc(\n",
    "    LAI_functions.count_valid_sequences, \n",
    "    mean_LAI_slope_masked,  # Assuming variable name\n",
    "    input_core_dims=[[\"dayofyear\"]],\n",
    "    vectorize=True\n",
    ")\n",
    "# Mask the entire (dayofyear, lat, lon) location where seq_count > 1\n",
    "LAI_slope_single = mean_LAI_slope_masked.where(seq_count <= 1)\n",
    "LAI_daily_single = LAI_daily.where(seq_count <= 1)\n",
    "LAI_slope_daily = LAI_slopes_daily.where(seq_count <= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0eead8-14ef-431a-ab1e-ae87cfbe35ac",
   "metadata": {},
   "source": [
    "#### crit 1: identify pixels with only single peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d76fe-e10a-49e1-92a4-06c9a01db89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and apply function along the time axis\n",
    "LAI_slope_single_daily = LAI_slope_daily.groupby(\"time.year\").map(lambda x: xr.apply_ufunc(\n",
    "    LAI_functions.longest_non_nan_sequence, x,\n",
    "    input_core_dims=[[\"time\"]],  # Apply along time dimension\n",
    "    output_core_dims=[[\"time\"]],  # Output same shape\n",
    "    vectorize=True,  # Apply element-wise\n",
    "    dask=\"parallelized\",  # Enable parallelization if using dask\n",
    "    output_dtypes=[x.dtype]  # Keep dtype same as input\n",
    "))\n",
    "\n",
    "mean_LAI_single = xr.apply_ufunc(\n",
    "    LAI_functions.longest_non_nan_sequence, mean_LAI,\n",
    "    input_core_dims=[[\"dayofyear\"]],  # Apply function along 'dayofyear' axis\n",
    "    output_core_dims=[[\"dayofyear\"]],  # Maintain same dimension in output\n",
    "    vectorize=True,  # Apply function element-wise across lat/lon\n",
    "    dask=\"parallelized\",  # Enable parallel execution if using dask\n",
    "    output_dtypes=[mean_LAI.dtype]  # Keep data type same as input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a6d18-9354-4168-a92f-e1b110098dd8",
   "metadata": {},
   "source": [
    "#### crit 2: select maximum distance of LAImax between years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba98c7-5211-4a88-bafc-b30553489275",
   "metadata": {},
   "outputs": [],
   "source": [
    "### period for AUC: peak + 40 days\n",
    "### HS: 10 before peak - peak+40 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd366284-bbb2-4741-8d45-7f1fb66368d5",
   "metadata": {},
   "source": [
    "#### 3. calculate AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a20bd-8c6e-450e-8d1c-bdd6f19e2697",
   "metadata": {},
   "source": [
    "#### 4. plot random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cc509-7229-4887-a782-6c28da0c7cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2025a",
   "language": "python",
   "name": "npl-2025a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
